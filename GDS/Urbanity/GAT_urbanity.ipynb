{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:48:36.599168Z",
     "start_time": "2025-03-26T13:48:34.295947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import base packages\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "# Import external packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "from collections import Counter"
   ],
   "id": "d06340c4ea93e490",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:49:03.256909Z",
     "start_time": "2025-03-26T13:48:36.600199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Read nodes data\n",
    "paris_100m_nodes = gpd.read_file('https://figshare.com/ndownloader/files/39331691')\n",
    "paris_100m_nodes = paris_100m_nodes.set_index('osmid')\n",
    "\n",
    "# # Read edges data\n",
    "paris_100m_edges = pd.read_csv('https://figshare.com/ndownloader/files/39331688', index_col=0)\n",
    "\n",
    "# # Visualise columns of edge dataframe\n",
    "paris_100m_edges.head()[['osmid', 'highway', 'geometry']]"
   ],
   "id": "2cfc6194dccec21f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               osmid  \\\n",
       "0                                 (2569, 2569, 2569)   \n",
       "1           (34855904, 34855904, 34855904, 34855904)   \n",
       "2                     (40001847, 40001847, 40001847)   \n",
       "3  (41361996, 41361996, 41361996, 41361996, 58441...   \n",
       "4                             (370869408, 370869408)   \n",
       "\n",
       "                                             highway  \\\n",
       "0               ('tertiary', 'tertiary', 'tertiary')   \n",
       "1       ('primary', 'primary', 'primary', 'primary')   \n",
       "2            ('secondary', 'secondary', 'secondary')   \n",
       "3  ('secondary', 'secondary', 'secondary', 'secon...   \n",
       "4                             ('primary', 'primary')   \n",
       "\n",
       "                                            geometry  \n",
       "0  LINESTRING (2.3694427 48.8215316, 2.3694159 48...  \n",
       "1  LINESTRING (2.3694427 48.8215316, 2.3697313 48...  \n",
       "2  LINESTRING (2.3694427 48.8215316, 2.3693587 48...  \n",
       "3  LINESTRING (2.3694427 48.8215316, 2.3695358 48...  \n",
       "4  LINESTRING (2.3694427 48.8215316, 2.3692848 48...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>osmid</th>\n",
       "      <th>highway</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2569, 2569, 2569)</td>\n",
       "      <td>('tertiary', 'tertiary', 'tertiary')</td>\n",
       "      <td>LINESTRING (2.3694427 48.8215316, 2.3694159 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(34855904, 34855904, 34855904, 34855904)</td>\n",
       "      <td>('primary', 'primary', 'primary', 'primary')</td>\n",
       "      <td>LINESTRING (2.3694427 48.8215316, 2.3697313 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(40001847, 40001847, 40001847)</td>\n",
       "      <td>('secondary', 'secondary', 'secondary')</td>\n",
       "      <td>LINESTRING (2.3694427 48.8215316, 2.3693587 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(41361996, 41361996, 41361996, 41361996, 58441...</td>\n",
       "      <td>('secondary', 'secondary', 'secondary', 'secon...</td>\n",
       "      <td>LINESTRING (2.3694427 48.8215316, 2.3695358 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(370869408, 370869408)</td>\n",
       "      <td>('primary', 'primary')</td>\n",
       "      <td>LINESTRING (2.3694427 48.8215316, 2.3692848 48...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:49:03.543967Z",
     "start_time": "2025-03-26T13:49:03.257804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Functions to Convert Edge to One Type\n",
    "\n",
    "def most_frequent(string_list):\n",
    "    \"\"\"Function to count the number of most occurring labels on an edge and relabel according to most frequent label.\n",
    "\n",
    "    Args:\n",
    "        string_list (list): A string of labels of road classification types\n",
    "\n",
    "    Returns:\n",
    "        str: The most common label along an edge\n",
    "    \"\"\"\n",
    "    occurence_count = Counter(string_list)\n",
    "    if occurence_count.most_common(1)[0][0] == 'unclassified':\n",
    "        try:\n",
    "            return occurence_count.most_common(2)[1][0]\n",
    "        except IndexError:\n",
    "            return 'unclassified'\n",
    "    else: \n",
    "        return occurence_count.most_common(1)[0][0]\n",
    "    \n",
    "def convert_labels(row):\n",
    "    \"\"\"Function to apply to each edge label, accounting for single entries. If only one entry, skips to next row.\n",
    "\n",
    "    Args:\n",
    "        row (str): Entries consisting of string of edge labels\n",
    "\n",
    "    Returns:\n",
    "        str: Most commmon label along an edge. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        list_of_roads = eval(row)\n",
    "        return most_frequent(list_of_roads)\n",
    "        \n",
    "    except NameError:\n",
    "        return row\n",
    "    \n",
    "# Create new relabelled column `Road Type`\n",
    "paris_100m_edges['Road Type'] = paris_100m_edges['highway'].apply(convert_labels)\n",
    "\n",
    "# Visualise new edge dataframe\n",
    "paris_100m_edges.head()[['osmid', 'highway', 'Road Type', 'geometry']]"
   ],
   "id": "8e4c29a9bf8da803",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               osmid  \\\n",
       "0                                 (2569, 2569, 2569)   \n",
       "1           (34855904, 34855904, 34855904, 34855904)   \n",
       "2                     (40001847, 40001847, 40001847)   \n",
       "3  (41361996, 41361996, 41361996, 41361996, 58441...   \n",
       "4                             (370869408, 370869408)   \n",
       "\n",
       "                                             highway  Road Type  \\\n",
       "0               ('tertiary', 'tertiary', 'tertiary')   tertiary   \n",
       "1       ('primary', 'primary', 'primary', 'primary')    primary   \n",
       "2            ('secondary', 'secondary', 'secondary')  secondary   \n",
       "3  ('secondary', 'secondary', 'secondary', 'secon...  secondary   \n",
       "4                             ('primary', 'primary')    primary   \n",
       "\n",
       "                                            geometry  \n",
       "0  LINESTRING (2.3694427 48.8215316, 2.3694159 48...  \n",
       "1  LINESTRING (2.3694427 48.8215316, 2.3697313 48...  \n",
       "2  LINESTRING (2.3694427 48.8215316, 2.3693587 48...  \n",
       "3  LINESTRING (2.3694427 48.8215316, 2.3695358 48...  \n",
       "4  LINESTRING (2.3694427 48.8215316, 2.3692848 48...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>osmid</th>\n",
       "      <th>highway</th>\n",
       "      <th>Road Type</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2569, 2569, 2569)</td>\n",
       "      <td>('tertiary', 'tertiary', 'tertiary')</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>LINESTRING (2.3694427 48.8215316, 2.3694159 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(34855904, 34855904, 34855904, 34855904)</td>\n",
       "      <td>('primary', 'primary', 'primary', 'primary')</td>\n",
       "      <td>primary</td>\n",
       "      <td>LINESTRING (2.3694427 48.8215316, 2.3697313 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(40001847, 40001847, 40001847)</td>\n",
       "      <td>('secondary', 'secondary', 'secondary')</td>\n",
       "      <td>secondary</td>\n",
       "      <td>LINESTRING (2.3694427 48.8215316, 2.3693587 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(41361996, 41361996, 41361996, 41361996, 58441...</td>\n",
       "      <td>('secondary', 'secondary', 'secondary', 'secon...</td>\n",
       "      <td>secondary</td>\n",
       "      <td>LINESTRING (2.3694427 48.8215316, 2.3695358 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(370869408, 370869408)</td>\n",
       "      <td>('primary', 'primary')</td>\n",
       "      <td>primary</td>\n",
       "      <td>LINESTRING (2.3694427 48.8215316, 2.3692848 48...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:49:03.590140Z",
     "start_time": "2025-03-26T13:49:03.544650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Relabel categories according to OSM TagInfo definition\n",
    "paris_100m_edges['Road Type'] = paris_100m_edges['Road Type'].replace({'service':5,\n",
    "                                                            'services':5,\n",
    "                                                            'residential':4,\n",
    "                                                            'primary':1,\n",
    "                                                            'secondary':2,\n",
    "                                                            'tertiary':2,\n",
    "                                                            'unclassified':4,\n",
    "                                                            'primary_link':3,\n",
    "                                                            'secondary_link':3,\n",
    "                                                            'trunk':2,\n",
    "                                                            'motorway_link':3,\n",
    "                                                            'motorway':1,\n",
    "                                                            'trunk_link':3,\n",
    "                                                            'tertiary_link':3,\n",
    "                                                            'living_street':4,\n",
    "                                                            'footway':4,\n",
    "                                                            'cycleway':4,\n",
    "                                                            'pedestrian':4,\n",
    "                                                            'bus_stop':3,\n",
    "                                                            'path':2,\n",
    "                                                            'track':5,\n",
    "                                                            'busway':3,\n",
    "                                                            'crossing':4\n",
    "                                                            })\n",
    "\n",
    "# Remove roads under construction, and with no description on OSM TagInfo (e.g., steps).\n",
    "paris_100m_edges = paris_100m_edges[(paris_100m_edges['Road Type']!='construction') & (paris_100m_edges['Road Type']!='steps')]\n",
    "\n",
    "# We show the final number of counts for each category\n",
    "paris_100m_edges['Road Type'].value_counts()"
   ],
   "id": "af9e6d6c8a4aa79c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Road Type\n",
       "4    23412\n",
       "5    12980\n",
       "2    10505\n",
       "1     6344\n",
       "3      970\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:49:03.595688Z",
     "start_time": "2025-03-26T13:49:03.591462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare target label\n",
    "label = paris_100m_edges['Road Type'].values - 1\n",
    "label = label.astype(float)\n",
    "label"
   ],
   "id": "6ff2fb753f147dab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., ..., 4., 4., 4.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:49:03.624277Z",
     "start_time": "2025-03-26T13:49:03.596287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare edge index. First match with index position then convert to torch tensor. \n",
    "node_to_id = {}\n",
    "for i,node in enumerate(paris_100m_nodes.index):\n",
    "    node_to_id[node] = i\n",
    "    \n",
    "# Second, create tensor list for start and end nodes\n",
    "start_node = [node_to_id[i] for i in paris_100m_edges['u'].values]\n",
    "end_node = [node_to_id[i] for i in paris_100m_edges['v'].values]\n",
    "start = torch.Tensor(start_node).to(torch.long)\n",
    "end = torch.Tensor(end_node).to(torch.long)\n",
    "edge_index = torch.stack([start, end], dim=0)\n",
    "\n",
    "# Show edge_index\n",
    "edge_index"
   ],
   "id": "4de2669a2990200d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,  ..., 18893, 18894, 18895],\n",
       "        [    1,   225, 11407,  ..., 16504, 17681, 10395]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:49:03.637397Z",
     "start_time": "2025-03-26T13:49:03.624981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Subset dataframe to features of interest\n",
    "x = np.array(paris_100m_nodes.iloc[:,:-1].values)\n",
    "\n",
    "# Normalise features\n",
    "scale = StandardScaler()\n",
    "x = scale.fit_transform(x)\n",
    "\n",
    "print(x.shape)"
   ],
   "id": "c6698554b7dcb5f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18896, 39)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:49:03.711795Z",
     "start_time": "2025-03-26T13:49:03.638082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import torch, training modules and classes\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set manual seed = 0 to allow reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create range of numbers for edge index list(0,1,..., no_of_edges) for subsetting and random split \n",
    "indices = list(range(len(paris_100m_edges)))\n",
    "    \n",
    "class MyDataset(InMemoryDataset):\n",
    "    def __init__(self, edge_index, attribute_data, edges_data, x, label, indices, transform=None):\n",
    "        super(MyDataset, self).__init__('.', transform, None, None)\n",
    "\n",
    "        data = Data(edge_index=edge_index)\n",
    "        \n",
    "        data.num_nodes = len(attribute_data)\n",
    "        data.num_edges = len(edges_data)\n",
    "        \n",
    "        # embedding \n",
    "        data.x = torch.from_numpy(x).type(torch.float32)\n",
    "        \n",
    "        # labels\n",
    "        y = torch.from_numpy(label).type(torch.long)\n",
    "        data.y = y.clone().detach()\n",
    "        \n",
    "        data.num_classes = 5 # five main highway categories\n",
    "\n",
    "        # splitting the data into train, validation and test\n",
    "        X_train, X_test, y_train, y_test, indices_train, indices_test, = train_test_split(edge_index.T, \n",
    "                                                                                        label,\n",
    "                                                                                        indices,\n",
    "                                                                                        test_size=0.1, \n",
    "                                                                                        random_state=99)\n",
    "        \n",
    "        \n",
    "        # create train and test masks for data\n",
    "        train_mask = torch.zeros(data.num_edges, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(data.num_edges, dtype=torch.bool)\n",
    "        train_mask[indices_train] = True\n",
    "        test_mask[indices_test] = True\n",
    "        data['train_mask'] = train_mask\n",
    "        data['test_mask'] = test_mask\n",
    "\n",
    "        self.data, self.slices = self.collate([data])\n",
    "\n",
    "def _download(self):\n",
    "    return\n",
    "\n",
    "def _process(self):\n",
    "    return\n",
    "\n",
    "def __repr__(self):\n",
    "    return '{}()'.format(self.__class__.__name__)\n",
    "\n",
    "# Set torch device to cuda if available or else cpu\n",
    "#device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Instantiate Dataset \n",
    "dataset = MyDataset(edge_index, paris_100m_nodes, paris_100m_edges, x, label, indices)\n",
    "data = dataset[0]\n",
    "data = data.to(device)\n",
    "\n",
    "# Show data object\n",
    "data"
   ],
   "id": "144e8196cfcec716",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 54211], num_nodes=18896, num_edges=54211, x=[18896, 39], y=[54211], num_classes=5, train_mask=[54211], test_mask=[54211])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:49:44.805181Z",
     "start_time": "2025-03-26T13:49:44.795201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import helper and utility functions from torch. Import GCNConv class. \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Define model hyper parameters\n",
    "HIDDEN_DIM = 64\n",
    "LR = 0.01\n",
    "EPOCH = 500\n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, hidden_layer, data, heads=4):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.gat1 = GATv2Conv(data.num_features, hidden_layer, heads=heads)\n",
    "        self.gat2 = GATv2Conv(hidden_layer*heads, hidden_layer, heads=1)\n",
    "        self.linear = nn.Linear(hidden_layer*2, data.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        out= self.gat1(x, edge_index)\n",
    "        out = F.elu(out)\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "        out = self.gat2(out, edge_index)\n",
    "\n",
    "        src, dst = edge_index\n",
    "        out = torch.concat((out[src], out[dst]),dim=1)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return F.softmax(out, dim=1)\n",
    "\n",
    "# Initialise GCN model instance\n",
    "model = GATv2(HIDDEN_DIM, data).to(device) \n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(model)"
   ],
   "id": "a8fbe19bd8cb015e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATv2(\n",
      "  (gat1): GATv2Conv(39, 64, heads=4)\n",
      "  (gat2): GATv2Conv(256, 64, heads=1)\n",
      "  (linear): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:49:48.140511Z",
     "start_time": "2025-03-26T13:49:47.643671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = model(data)\n",
    "train_loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "print(f'Training Loss: {train_loss}')\n",
    "training_loss = torch.sum(train_loss)\n",
    "print(f'Training Loss: {training_loss}')\n",
    "train_loss.backward()"
   ],
   "id": "9686f1caef6b65d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.60616934299469\n",
      "Training Loss: 1.60616934299469\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:53:05.689850Z",
     "start_time": "2025-03-26T13:50:40.302491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "epoch_train_val = {}\n",
    "\n",
    "for epoch in range(1,EPOCH+1):\n",
    "    #print(f'Epoch: {epoch}/{EPOCH}')\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    train_loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    training_loss = torch.sum(train_loss)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    pred = model(data)\n",
    "    test_loss = criterion(pred[data.test_mask], data.y[data.test_mask])\n",
    "    test_loss = torch.sum(test_loss)\n",
    "    # scheduler.step(test_loss)\n",
    "\n",
    "    epoch_train_val[epoch] = {'training_loss':training_loss.item(), 'test_loss' :test_loss.item()}\n",
    "\n",
    "    print(f'Epoch: {epoch}/{EPOCH}- Training Loss: {training_loss}; Testing Loss: {test_loss}')\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        pred = model(data).argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "        acc = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "        print(f'Validation accuracy: {acc}.')"
   ],
   "id": "9081bac64d05f8bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500\n",
      "Epoch: 1/500- Training Loss: 1.606338381767273; Testing Loss: 1.4349045753479004\n",
      "Epoch: 2/500\n",
      "Epoch: 2/500- Training Loss: 1.4353446960449219; Testing Loss: 1.3942463397979736\n",
      "Epoch: 3/500\n",
      "Epoch: 3/500- Training Loss: 1.39412522315979; Testing Loss: 1.3757297992706299\n",
      "Epoch: 4/500\n",
      "Epoch: 4/500- Training Loss: 1.3781431913375854; Testing Loss: 1.3691922426223755\n",
      "Epoch: 5/500\n",
      "Epoch: 5/500- Training Loss: 1.3729184865951538; Testing Loss: 1.363065242767334\n",
      "Epoch: 6/500\n",
      "Epoch: 6/500- Training Loss: 1.3631175756454468; Testing Loss: 1.35297691822052\n",
      "Epoch: 7/500\n",
      "Epoch: 7/500- Training Loss: 1.3524028062820435; Testing Loss: 1.3524097204208374\n",
      "Epoch: 8/500\n",
      "Epoch: 8/500- Training Loss: 1.353668451309204; Testing Loss: 1.3588742017745972\n",
      "Epoch: 9/500\n",
      "Epoch: 9/500- Training Loss: 1.355810523033142; Testing Loss: 1.3535869121551514\n",
      "Epoch: 10/500\n",
      "Epoch: 10/500- Training Loss: 1.3497400283813477; Testing Loss: 1.3481661081314087\n",
      "Epoch: 11/500\n",
      "Epoch: 11/500- Training Loss: 1.3444688320159912; Testing Loss: 1.344524621963501\n",
      "Epoch: 12/500\n",
      "Epoch: 12/500- Training Loss: 1.342442512512207; Testing Loss: 1.3429667949676514\n",
      "Epoch: 13/500\n",
      "Epoch: 13/500- Training Loss: 1.3411815166473389; Testing Loss: 1.3406707048416138\n",
      "Epoch: 14/500\n",
      "Epoch: 14/500- Training Loss: 1.3394231796264648; Testing Loss: 1.3364195823669434\n",
      "Epoch: 15/500\n",
      "Epoch: 15/500- Training Loss: 1.3373640775680542; Testing Loss: 1.334958553314209\n",
      "Epoch: 16/500\n",
      "Epoch: 16/500- Training Loss: 1.3340011835098267; Testing Loss: 1.3346937894821167\n",
      "Epoch: 17/500\n",
      "Epoch: 17/500- Training Loss: 1.3313894271850586; Testing Loss: 1.3348616361618042\n",
      "Epoch: 18/500\n",
      "Epoch: 18/500- Training Loss: 1.3304589986801147; Testing Loss: 1.3332583904266357\n",
      "Epoch: 19/500\n",
      "Epoch: 19/500- Training Loss: 1.3296899795532227; Testing Loss: 1.3289974927902222\n",
      "Epoch: 20/500\n",
      "Epoch: 20/500- Training Loss: 1.3248021602630615; Testing Loss: 1.3269623517990112\n",
      "Validation accuracy: 0.5715603098487643.\n",
      "Epoch: 21/500\n",
      "Epoch: 21/500- Training Loss: 1.323004126548767; Testing Loss: 1.323411226272583\n",
      "Epoch: 22/500\n",
      "Epoch: 22/500- Training Loss: 1.3198492527008057; Testing Loss: 1.3217819929122925\n",
      "Epoch: 23/500\n",
      "Epoch: 23/500- Training Loss: 1.3174762725830078; Testing Loss: 1.3185490369796753\n",
      "Epoch: 24/500\n",
      "Epoch: 24/500- Training Loss: 1.3158535957336426; Testing Loss: 1.3157397508621216\n",
      "Epoch: 25/500\n",
      "Epoch: 25/500- Training Loss: 1.3151928186416626; Testing Loss: 1.3143811225891113\n",
      "Epoch: 26/500\n",
      "Epoch: 26/500- Training Loss: 1.314041256904602; Testing Loss: 1.3134809732437134\n",
      "Epoch: 27/500\n",
      "Epoch: 27/500- Training Loss: 1.3119210004806519; Testing Loss: 1.3122303485870361\n",
      "Epoch: 28/500\n",
      "Epoch: 28/500- Training Loss: 1.3091236352920532; Testing Loss: 1.3096718788146973\n",
      "Epoch: 29/500\n",
      "Epoch: 29/500- Training Loss: 1.3091703653335571; Testing Loss: 1.3078991174697876\n",
      "Epoch: 30/500\n",
      "Epoch: 30/500- Training Loss: 1.3079862594604492; Testing Loss: 1.3072881698608398\n",
      "Epoch: 31/500\n",
      "Epoch: 31/500- Training Loss: 1.3041597604751587; Testing Loss: 1.306682825088501\n",
      "Epoch: 32/500\n",
      "Epoch: 32/500- Training Loss: 1.3035225868225098; Testing Loss: 1.306174635887146\n",
      "Epoch: 33/500\n",
      "Epoch: 33/500- Training Loss: 1.3018382787704468; Testing Loss: 1.3051326274871826\n",
      "Epoch: 34/500\n",
      "Epoch: 34/500- Training Loss: 1.3018853664398193; Testing Loss: 1.3035515546798706\n",
      "Epoch: 35/500\n",
      "Epoch: 35/500- Training Loss: 1.300191044807434; Testing Loss: 1.3029327392578125\n",
      "Epoch: 36/500\n",
      "Epoch: 36/500- Training Loss: 1.299237608909607; Testing Loss: 1.302116870880127\n",
      "Epoch: 37/500\n",
      "Epoch: 37/500- Training Loss: 1.2985440492630005; Testing Loss: 1.2997658252716064\n",
      "Epoch: 38/500\n",
      "Epoch: 38/500- Training Loss: 1.2991453409194946; Testing Loss: 1.297979474067688\n",
      "Epoch: 39/500\n",
      "Epoch: 39/500- Training Loss: 1.2949779033660889; Testing Loss: 1.2951886653900146\n",
      "Epoch: 40/500\n",
      "Epoch: 40/500- Training Loss: 1.2949243783950806; Testing Loss: 1.2932114601135254\n",
      "Validation accuracy: 0.6093692364441166.\n",
      "Epoch: 41/500\n",
      "Epoch: 41/500- Training Loss: 1.2933077812194824; Testing Loss: 1.293736219406128\n",
      "Epoch: 42/500\n",
      "Epoch: 42/500- Training Loss: 1.2929205894470215; Testing Loss: 1.293294072151184\n",
      "Epoch: 43/500\n",
      "Epoch: 43/500- Training Loss: 1.2933557033538818; Testing Loss: 1.2937610149383545\n",
      "Epoch: 44/500\n",
      "Epoch: 44/500- Training Loss: 1.2929567098617554; Testing Loss: 1.2927777767181396\n",
      "Epoch: 45/500\n",
      "Epoch: 45/500- Training Loss: 1.2908061742782593; Testing Loss: 1.2929964065551758\n",
      "Epoch: 46/500\n",
      "Epoch: 46/500- Training Loss: 1.2905817031860352; Testing Loss: 1.2922556400299072\n",
      "Epoch: 47/500\n",
      "Epoch: 47/500- Training Loss: 1.287378191947937; Testing Loss: 1.2906910181045532\n",
      "Epoch: 48/500\n",
      "Epoch: 48/500- Training Loss: 1.287483811378479; Testing Loss: 1.2900291681289673\n",
      "Epoch: 49/500\n",
      "Epoch: 49/500- Training Loss: 1.284177541732788; Testing Loss: 1.2879644632339478\n",
      "Epoch: 50/500\n",
      "Epoch: 50/500- Training Loss: 1.2869939804077148; Testing Loss: 1.286750078201294\n",
      "Epoch: 51/500\n",
      "Epoch: 51/500- Training Loss: 1.2845979928970337; Testing Loss: 1.2851258516311646\n",
      "Epoch: 52/500\n",
      "Epoch: 52/500- Training Loss: 1.2837388515472412; Testing Loss: 1.2850919961929321\n",
      "Epoch: 53/500\n",
      "Epoch: 53/500- Training Loss: 1.2820379734039307; Testing Loss: 1.2853803634643555\n",
      "Epoch: 54/500\n",
      "Epoch: 54/500- Training Loss: 1.28252375125885; Testing Loss: 1.2843660116195679\n",
      "Epoch: 55/500\n",
      "Epoch: 55/500- Training Loss: 1.2827008962631226; Testing Loss: 1.283779501914978\n",
      "Epoch: 56/500\n",
      "Epoch: 56/500- Training Loss: 1.279889702796936; Testing Loss: 1.283713459968567\n",
      "Epoch: 57/500\n",
      "Epoch: 57/500- Training Loss: 1.2803833484649658; Testing Loss: 1.2847018241882324\n",
      "Epoch: 58/500\n",
      "Epoch: 58/500- Training Loss: 1.2796376943588257; Testing Loss: 1.2856361865997314\n",
      "Epoch: 59/500\n",
      "Epoch: 59/500- Training Loss: 1.2779453992843628; Testing Loss: 1.2835983037948608\n",
      "Epoch: 60/500\n",
      "Epoch: 60/500- Training Loss: 1.2782645225524902; Testing Loss: 1.2826343774795532\n",
      "Validation accuracy: 0.616746587974917.\n",
      "Epoch: 61/500\n",
      "Epoch: 61/500- Training Loss: 1.2798105478286743; Testing Loss: 1.2836437225341797\n",
      "Epoch: 62/500\n",
      "Epoch: 62/500- Training Loss: 1.2777414321899414; Testing Loss: 1.2819688320159912\n",
      "Epoch: 63/500\n",
      "Epoch: 63/500- Training Loss: 1.2768267393112183; Testing Loss: 1.2813013792037964\n",
      "Epoch: 64/500\n",
      "Epoch: 64/500- Training Loss: 1.2761577367782593; Testing Loss: 1.2802375555038452\n",
      "Epoch: 65/500\n",
      "Epoch: 65/500- Training Loss: 1.2754607200622559; Testing Loss: 1.2803938388824463\n",
      "Epoch: 66/500\n",
      "Epoch: 66/500- Training Loss: 1.2736893892288208; Testing Loss: 1.2794129848480225\n",
      "Epoch: 67/500\n",
      "Epoch: 67/500- Training Loss: 1.2749196290969849; Testing Loss: 1.276090145111084\n",
      "Epoch: 68/500\n",
      "Epoch: 68/500- Training Loss: 1.270355463027954; Testing Loss: 1.275787353515625\n",
      "Epoch: 69/500\n",
      "Epoch: 69/500- Training Loss: 1.2706199884414673; Testing Loss: 1.2745236158370972\n",
      "Epoch: 70/500\n",
      "Epoch: 70/500- Training Loss: 1.2708940505981445; Testing Loss: 1.2739626169204712\n",
      "Epoch: 71/500\n",
      "Epoch: 71/500- Training Loss: 1.2704764604568481; Testing Loss: 1.2734202146530151\n",
      "Epoch: 72/500\n",
      "Epoch: 72/500- Training Loss: 1.268385887145996; Testing Loss: 1.2718805074691772\n",
      "Epoch: 73/500\n",
      "Epoch: 73/500- Training Loss: 1.2673735618591309; Testing Loss: 1.2724223136901855\n",
      "Epoch: 74/500\n",
      "Epoch: 74/500- Training Loss: 1.2672345638275146; Testing Loss: 1.2732045650482178\n",
      "Epoch: 75/500\n",
      "Epoch: 75/500- Training Loss: 1.267723798751831; Testing Loss: 1.2718156576156616\n",
      "Epoch: 76/500\n",
      "Epoch: 76/500- Training Loss: 1.2656195163726807; Testing Loss: 1.268859624862671\n",
      "Epoch: 77/500\n",
      "Epoch: 77/500- Training Loss: 1.262533187866211; Testing Loss: 1.2672467231750488\n",
      "Epoch: 78/500\n",
      "Epoch: 78/500- Training Loss: 1.2654186487197876; Testing Loss: 1.2656118869781494\n",
      "Epoch: 79/500\n",
      "Epoch: 79/500- Training Loss: 1.2634495496749878; Testing Loss: 1.2652032375335693\n",
      "Epoch: 80/500\n",
      "Epoch: 80/500- Training Loss: 1.2601981163024902; Testing Loss: 1.2675509452819824\n",
      "Validation accuracy: 0.633898930284028.\n",
      "Epoch: 81/500\n",
      "Epoch: 81/500- Training Loss: 1.2637287378311157; Testing Loss: 1.267623782157898\n",
      "Epoch: 82/500\n",
      "Epoch: 82/500- Training Loss: 1.262613296508789; Testing Loss: 1.2671862840652466\n",
      "Epoch: 83/500\n",
      "Epoch: 83/500- Training Loss: 1.260168194770813; Testing Loss: 1.2648073434829712\n",
      "Epoch: 84/500\n",
      "Epoch: 84/500- Training Loss: 1.2583686113357544; Testing Loss: 1.2639451026916504\n",
      "Epoch: 85/500\n",
      "Epoch: 85/500- Training Loss: 1.2594842910766602; Testing Loss: 1.2646375894546509\n",
      "Epoch: 86/500\n",
      "Epoch: 86/500- Training Loss: 1.2572176456451416; Testing Loss: 1.2642667293548584\n",
      "Epoch: 87/500\n",
      "Epoch: 87/500- Training Loss: 1.2581883668899536; Testing Loss: 1.2637519836425781\n",
      "Epoch: 88/500\n",
      "Epoch: 88/500- Training Loss: 1.2565442323684692; Testing Loss: 1.2677781581878662\n",
      "Epoch: 89/500\n",
      "Epoch: 89/500- Training Loss: 1.2571067810058594; Testing Loss: 1.2661316394805908\n",
      "Epoch: 90/500\n",
      "Epoch: 90/500- Training Loss: 1.2540861368179321; Testing Loss: 1.2624140977859497\n",
      "Epoch: 91/500\n",
      "Epoch: 91/500- Training Loss: 1.2545008659362793; Testing Loss: 1.2630243301391602\n",
      "Epoch: 92/500\n",
      "Epoch: 92/500- Training Loss: 1.2553099393844604; Testing Loss: 1.2628064155578613\n",
      "Epoch: 93/500\n",
      "Epoch: 93/500- Training Loss: 1.2563081979751587; Testing Loss: 1.258113145828247\n",
      "Epoch: 94/500\n",
      "Epoch: 94/500- Training Loss: 1.2529404163360596; Testing Loss: 1.2595113515853882\n",
      "Epoch: 95/500\n",
      "Epoch: 95/500- Training Loss: 1.2530497312545776; Testing Loss: 1.263032078742981\n",
      "Epoch: 96/500\n",
      "Epoch: 96/500- Training Loss: 1.2524532079696655; Testing Loss: 1.2621972560882568\n",
      "Epoch: 97/500\n",
      "Epoch: 97/500- Training Loss: 1.2517662048339844; Testing Loss: 1.260611891746521\n",
      "Epoch: 98/500\n",
      "Epoch: 98/500- Training Loss: 1.253581166267395; Testing Loss: 1.2574814558029175\n",
      "Epoch: 99/500\n",
      "Epoch: 99/500- Training Loss: 1.2501825094223022; Testing Loss: 1.255293607711792\n",
      "Epoch: 100/500\n",
      "Epoch: 100/500- Training Loss: 1.2501370906829834; Testing Loss: 1.2560080289840698\n",
      "Validation accuracy: 0.6462559940981187.\n",
      "Epoch: 101/500\n",
      "Epoch: 101/500- Training Loss: 1.2490262985229492; Testing Loss: 1.2552129030227661\n",
      "Epoch: 102/500\n",
      "Epoch: 102/500- Training Loss: 1.2471795082092285; Testing Loss: 1.25642991065979\n",
      "Epoch: 103/500\n",
      "Epoch: 103/500- Training Loss: 1.2458527088165283; Testing Loss: 1.2584155797958374\n",
      "Epoch: 104/500\n",
      "Epoch: 104/500- Training Loss: 1.2452380657196045; Testing Loss: 1.2564069032669067\n",
      "Epoch: 105/500\n",
      "Epoch: 105/500- Training Loss: 1.2458021640777588; Testing Loss: 1.2533584833145142\n",
      "Epoch: 106/500\n",
      "Epoch: 106/500- Training Loss: 1.2453346252441406; Testing Loss: 1.2562342882156372\n",
      "Epoch: 107/500\n",
      "Epoch: 107/500- Training Loss: 1.2449560165405273; Testing Loss: 1.2544015645980835\n",
      "Epoch: 108/500\n",
      "Epoch: 108/500- Training Loss: 1.242782711982727; Testing Loss: 1.252503752708435\n",
      "Epoch: 109/500\n",
      "Epoch: 109/500- Training Loss: 1.2422734498977661; Testing Loss: 1.2545738220214844\n",
      "Epoch: 110/500\n",
      "Epoch: 110/500- Training Loss: 1.240053653717041; Testing Loss: 1.2537503242492676\n",
      "Epoch: 111/500\n",
      "Epoch: 111/500- Training Loss: 1.23833429813385; Testing Loss: 1.252265214920044\n",
      "Epoch: 112/500\n",
      "Epoch: 112/500- Training Loss: 1.2394219636917114; Testing Loss: 1.250837802886963\n",
      "Epoch: 113/500\n",
      "Epoch: 113/500- Training Loss: 1.237436056137085; Testing Loss: 1.251399278640747\n",
      "Epoch: 114/500\n",
      "Epoch: 114/500- Training Loss: 1.2377451658248901; Testing Loss: 1.2447373867034912\n",
      "Epoch: 115/500\n",
      "Epoch: 115/500- Training Loss: 1.2349680662155151; Testing Loss: 1.2421331405639648\n",
      "Epoch: 116/500\n",
      "Epoch: 116/500- Training Loss: 1.2360138893127441; Testing Loss: 1.2465810775756836\n",
      "Epoch: 117/500\n",
      "Epoch: 117/500- Training Loss: 1.2354133129119873; Testing Loss: 1.248340129852295\n",
      "Epoch: 118/500\n",
      "Epoch: 118/500- Training Loss: 1.2376657724380493; Testing Loss: 1.2476242780685425\n",
      "Epoch: 119/500\n",
      "Epoch: 119/500- Training Loss: 1.2331711053848267; Testing Loss: 1.2457826137542725\n",
      "Epoch: 120/500\n",
      "Epoch: 120/500- Training Loss: 1.2315809726715088; Testing Loss: 1.2501142024993896\n",
      "Validation accuracy: 0.6517890077462191.\n",
      "Epoch: 121/500\n",
      "Epoch: 121/500- Training Loss: 1.2328524589538574; Testing Loss: 1.2438490390777588\n",
      "Epoch: 122/500\n",
      "Epoch: 122/500- Training Loss: 1.2298939228057861; Testing Loss: 1.242154598236084\n",
      "Epoch: 123/500\n",
      "Epoch: 123/500- Training Loss: 1.2290170192718506; Testing Loss: 1.2407221794128418\n",
      "Epoch: 124/500\n",
      "Epoch: 124/500- Training Loss: 1.2279472351074219; Testing Loss: 1.2400044202804565\n",
      "Epoch: 125/500\n",
      "Epoch: 125/500- Training Loss: 1.2266100645065308; Testing Loss: 1.242029070854187\n",
      "Epoch: 126/500\n",
      "Epoch: 126/500- Training Loss: 1.2267484664916992; Testing Loss: 1.2495405673980713\n",
      "Epoch: 127/500\n",
      "Epoch: 127/500- Training Loss: 1.2294015884399414; Testing Loss: 1.2446404695510864\n",
      "Epoch: 128/500\n",
      "Epoch: 128/500- Training Loss: 1.2307648658752441; Testing Loss: 1.2427703142166138\n",
      "Epoch: 129/500\n",
      "Epoch: 129/500- Training Loss: 1.2301864624023438; Testing Loss: 1.2391726970672607\n",
      "Epoch: 130/500\n",
      "Epoch: 130/500- Training Loss: 1.2248709201812744; Testing Loss: 1.2416515350341797\n",
      "Epoch: 131/500\n",
      "Epoch: 131/500- Training Loss: 1.2267860174179077; Testing Loss: 1.2398792505264282\n",
      "Epoch: 132/500\n",
      "Epoch: 132/500- Training Loss: 1.2259601354599; Testing Loss: 1.23918616771698\n",
      "Epoch: 133/500\n",
      "Epoch: 133/500- Training Loss: 1.2258784770965576; Testing Loss: 1.23909592628479\n",
      "Epoch: 134/500\n",
      "Epoch: 134/500- Training Loss: 1.2218883037567139; Testing Loss: 1.2433780431747437\n",
      "Epoch: 135/500\n",
      "Epoch: 135/500- Training Loss: 1.2237032651901245; Testing Loss: 1.238847017288208\n",
      "Epoch: 136/500\n",
      "Epoch: 136/500- Training Loss: 1.2222208976745605; Testing Loss: 1.2333623170852661\n",
      "Epoch: 137/500\n",
      "Epoch: 137/500- Training Loss: 1.223063349723816; Testing Loss: 1.2355695962905884\n",
      "Epoch: 138/500\n",
      "Epoch: 138/500- Training Loss: 1.2207854986190796; Testing Loss: 1.2382723093032837\n",
      "Epoch: 139/500\n",
      "Epoch: 139/500- Training Loss: 1.2227150201797485; Testing Loss: 1.2352757453918457\n",
      "Epoch: 140/500\n",
      "Epoch: 140/500- Training Loss: 1.2200828790664673; Testing Loss: 1.233280897140503\n",
      "Validation accuracy: 0.6689413500553302.\n",
      "Epoch: 141/500\n",
      "Epoch: 141/500- Training Loss: 1.2178430557250977; Testing Loss: 1.2359619140625\n",
      "Epoch: 142/500\n",
      "Epoch: 142/500- Training Loss: 1.2194181680679321; Testing Loss: 1.2366646528244019\n",
      "Epoch: 143/500\n",
      "Epoch: 143/500- Training Loss: 1.2173564434051514; Testing Loss: 1.2427208423614502\n",
      "Epoch: 144/500\n",
      "Epoch: 144/500- Training Loss: 1.2217668294906616; Testing Loss: 1.2370470762252808\n",
      "Epoch: 145/500\n",
      "Epoch: 145/500- Training Loss: 1.2165864706039429; Testing Loss: 1.2382725477218628\n",
      "Epoch: 146/500\n",
      "Epoch: 146/500- Training Loss: 1.2227445840835571; Testing Loss: 1.2306804656982422\n",
      "Epoch: 147/500\n",
      "Epoch: 147/500- Training Loss: 1.2145675420761108; Testing Loss: 1.2383097410202026\n",
      "Epoch: 148/500\n",
      "Epoch: 148/500- Training Loss: 1.220223307609558; Testing Loss: 1.2365117073059082\n",
      "Epoch: 149/500\n",
      "Epoch: 149/500- Training Loss: 1.2181683778762817; Testing Loss: 1.2349433898925781\n",
      "Epoch: 150/500\n",
      "Epoch: 150/500- Training Loss: 1.2134599685668945; Testing Loss: 1.2323951721191406\n",
      "Epoch: 151/500\n",
      "Epoch: 151/500- Training Loss: 1.2142006158828735; Testing Loss: 1.23634672164917\n",
      "Epoch: 152/500\n",
      "Epoch: 152/500- Training Loss: 1.2140058279037476; Testing Loss: 1.2390109300613403\n",
      "Epoch: 153/500\n",
      "Epoch: 153/500- Training Loss: 1.219974160194397; Testing Loss: 1.232373833656311\n",
      "Epoch: 154/500\n",
      "Epoch: 154/500- Training Loss: 1.2136021852493286; Testing Loss: 1.2345640659332275\n",
      "Epoch: 155/500\n",
      "Epoch: 155/500- Training Loss: 1.2169725894927979; Testing Loss: 1.233371615409851\n",
      "Epoch: 156/500\n",
      "Epoch: 156/500- Training Loss: 1.2151066064834595; Testing Loss: 1.2319186925888062\n",
      "Epoch: 157/500\n",
      "Epoch: 157/500- Training Loss: 1.21359384059906; Testing Loss: 1.230607032775879\n",
      "Epoch: 158/500\n",
      "Epoch: 158/500- Training Loss: 1.2130213975906372; Testing Loss: 1.2292218208312988\n",
      "Epoch: 159/500\n",
      "Epoch: 159/500- Training Loss: 1.2124080657958984; Testing Loss: 1.232566475868225\n",
      "Epoch: 160/500\n",
      "Epoch: 160/500- Training Loss: 1.2142395973205566; Testing Loss: 1.229362964630127\n",
      "Validation accuracy: 0.6728144596090003.\n",
      "Epoch: 161/500\n",
      "Epoch: 161/500- Training Loss: 1.2055784463882446; Testing Loss: 1.2342841625213623\n",
      "Epoch: 162/500\n",
      "Epoch: 162/500- Training Loss: 1.213140845298767; Testing Loss: 1.2292388677597046\n",
      "Epoch: 163/500\n",
      "Epoch: 163/500- Training Loss: 1.2103360891342163; Testing Loss: 1.2287622690200806\n",
      "Epoch: 164/500\n",
      "Epoch: 164/500- Training Loss: 1.2077714204788208; Testing Loss: 1.231094241142273\n",
      "Epoch: 165/500\n",
      "Epoch: 165/500- Training Loss: 1.2093342542648315; Testing Loss: 1.229963779449463\n",
      "Epoch: 166/500\n",
      "Epoch: 166/500- Training Loss: 1.2081440687179565; Testing Loss: 1.2322343587875366\n",
      "Epoch: 167/500\n",
      "Epoch: 167/500- Training Loss: 1.2093985080718994; Testing Loss: 1.2258738279342651\n",
      "Epoch: 168/500\n",
      "Epoch: 168/500- Training Loss: 1.2027908563613892; Testing Loss: 1.2256337404251099\n",
      "Epoch: 169/500\n",
      "Epoch: 169/500- Training Loss: 1.20802640914917; Testing Loss: 1.2264913320541382\n",
      "Epoch: 170/500\n",
      "Epoch: 170/500- Training Loss: 1.2074822187423706; Testing Loss: 1.2264901399612427\n",
      "Epoch: 171/500\n",
      "Epoch: 171/500- Training Loss: 1.2046314477920532; Testing Loss: 1.2257411479949951\n",
      "Epoch: 172/500\n",
      "Epoch: 172/500- Training Loss: 1.2061938047409058; Testing Loss: 1.2244291305541992\n",
      "Epoch: 173/500\n",
      "Epoch: 173/500- Training Loss: 1.2071373462677002; Testing Loss: 1.2226777076721191\n",
      "Epoch: 174/500\n",
      "Epoch: 174/500- Training Loss: 1.2023817300796509; Testing Loss: 1.2277215719223022\n",
      "Epoch: 175/500\n",
      "Epoch: 175/500- Training Loss: 1.20559823513031; Testing Loss: 1.22748863697052\n",
      "Epoch: 176/500\n",
      "Epoch: 176/500- Training Loss: 1.2046204805374146; Testing Loss: 1.2249292135238647\n",
      "Epoch: 177/500\n",
      "Epoch: 177/500- Training Loss: 1.2056080102920532; Testing Loss: 1.2250250577926636\n",
      "Epoch: 178/500\n",
      "Epoch: 178/500- Training Loss: 1.207221508026123; Testing Loss: 1.2219774723052979\n",
      "Epoch: 179/500\n",
      "Epoch: 179/500- Training Loss: 1.203092098236084; Testing Loss: 1.2337288856506348\n",
      "Epoch: 180/500\n",
      "Epoch: 180/500- Training Loss: 1.2123427391052246; Testing Loss: 1.2261836528778076\n",
      "Validation accuracy: 0.6766875691626706.\n",
      "Epoch: 181/500\n",
      "Epoch: 181/500- Training Loss: 1.2038460969924927; Testing Loss: 1.2243355512619019\n",
      "Epoch: 182/500\n",
      "Epoch: 182/500- Training Loss: 1.2059130668640137; Testing Loss: 1.225113034248352\n",
      "Epoch: 183/500\n",
      "Epoch: 183/500- Training Loss: 1.2078237533569336; Testing Loss: 1.2186713218688965\n",
      "Epoch: 184/500\n",
      "Epoch: 184/500- Training Loss: 1.2023613452911377; Testing Loss: 1.2253050804138184\n",
      "Epoch: 185/500\n",
      "Epoch: 185/500- Training Loss: 1.2052931785583496; Testing Loss: 1.2231006622314453\n",
      "Epoch: 186/500\n",
      "Epoch: 186/500- Training Loss: 1.201758623123169; Testing Loss: 1.2214957475662231\n",
      "Epoch: 187/500\n",
      "Epoch: 187/500- Training Loss: 1.2004680633544922; Testing Loss: 1.2244832515716553\n",
      "Epoch: 188/500\n",
      "Epoch: 188/500- Training Loss: 1.2038633823394775; Testing Loss: 1.2229691743850708\n",
      "Epoch: 189/500\n",
      "Epoch: 189/500- Training Loss: 1.2054542303085327; Testing Loss: 1.2185786962509155\n",
      "Epoch: 190/500\n",
      "Epoch: 190/500- Training Loss: 1.1982955932617188; Testing Loss: 1.2201966047286987\n",
      "Epoch: 191/500\n",
      "Epoch: 191/500- Training Loss: 1.1979156732559204; Testing Loss: 1.2238229513168335\n",
      "Epoch: 192/500\n",
      "Epoch: 192/500- Training Loss: 1.2018048763275146; Testing Loss: 1.221666693687439\n",
      "Epoch: 193/500\n",
      "Epoch: 193/500- Training Loss: 1.198555827140808; Testing Loss: 1.2233294248580933\n",
      "Epoch: 194/500\n",
      "Epoch: 194/500- Training Loss: 1.2041897773742676; Testing Loss: 1.218956708908081\n",
      "Epoch: 195/500\n",
      "Epoch: 195/500- Training Loss: 1.197083830833435; Testing Loss: 1.2221847772598267\n",
      "Epoch: 196/500\n",
      "Epoch: 196/500- Training Loss: 1.1973085403442383; Testing Loss: 1.2210125923156738\n",
      "Epoch: 197/500\n",
      "Epoch: 197/500- Training Loss: 1.1992369890213013; Testing Loss: 1.2223063707351685\n",
      "Epoch: 198/500\n",
      "Epoch: 198/500- Training Loss: 1.197465181350708; Testing Loss: 1.2209064960479736\n",
      "Epoch: 199/500\n",
      "Epoch: 199/500- Training Loss: 1.1964197158813477; Testing Loss: 1.221213698387146\n",
      "Epoch: 200/500\n",
      "Epoch: 200/500- Training Loss: 1.1964558362960815; Testing Loss: 1.2227026224136353\n",
      "Validation accuracy: 0.6789007746219108.\n",
      "Epoch: 201/500\n",
      "Epoch: 201/500- Training Loss: 1.194288969039917; Testing Loss: 1.2180097103118896\n",
      "Epoch: 202/500\n",
      "Epoch: 202/500- Training Loss: 1.1929644346237183; Testing Loss: 1.2174072265625\n",
      "Epoch: 203/500\n",
      "Epoch: 203/500- Training Loss: 1.1956340074539185; Testing Loss: 1.2153288125991821\n",
      "Epoch: 204/500\n",
      "Epoch: 204/500- Training Loss: 1.1950855255126953; Testing Loss: 1.215949296951294\n",
      "Epoch: 205/500\n",
      "Epoch: 205/500- Training Loss: 1.197811484336853; Testing Loss: 1.211321234703064\n",
      "Epoch: 206/500\n",
      "Epoch: 206/500- Training Loss: 1.1895002126693726; Testing Loss: 1.2142242193222046\n",
      "Epoch: 207/500\n",
      "Epoch: 207/500- Training Loss: 1.190851092338562; Testing Loss: 1.2156896591186523\n",
      "Epoch: 208/500\n",
      "Epoch: 208/500- Training Loss: 1.1922041177749634; Testing Loss: 1.2156956195831299\n",
      "Epoch: 209/500\n",
      "Epoch: 209/500- Training Loss: 1.1921175718307495; Testing Loss: 1.2149704694747925\n",
      "Epoch: 210/500\n",
      "Epoch: 210/500- Training Loss: 1.1924183368682861; Testing Loss: 1.2136664390563965\n",
      "Epoch: 211/500\n",
      "Epoch: 211/500- Training Loss: 1.1908766031265259; Testing Loss: 1.2133458852767944\n",
      "Epoch: 212/500\n",
      "Epoch: 212/500- Training Loss: 1.1891018152236938; Testing Loss: 1.212146282196045\n",
      "Epoch: 213/500\n",
      "Epoch: 213/500- Training Loss: 1.191563606262207; Testing Loss: 1.2144054174423218\n",
      "Epoch: 214/500\n",
      "Epoch: 214/500- Training Loss: 1.1926250457763672; Testing Loss: 1.2115226984024048\n",
      "Epoch: 215/500\n",
      "Epoch: 215/500- Training Loss: 1.187778115272522; Testing Loss: 1.211495041847229\n",
      "Epoch: 216/500\n",
      "Epoch: 216/500- Training Loss: 1.1904420852661133; Testing Loss: 1.2083559036254883\n",
      "Epoch: 217/500\n",
      "Epoch: 217/500- Training Loss: 1.188362717628479; Testing Loss: 1.2112269401550293\n",
      "Epoch: 218/500\n",
      "Epoch: 218/500- Training Loss: 1.1895800828933716; Testing Loss: 1.209223747253418\n",
      "Epoch: 219/500\n",
      "Epoch: 219/500- Training Loss: 1.1885279417037964; Testing Loss: 1.212918758392334\n",
      "Epoch: 220/500\n",
      "Epoch: 220/500- Training Loss: 1.1906285285949707; Testing Loss: 1.2115064859390259\n",
      "Validation accuracy: 0.6907045370711914.\n",
      "Epoch: 221/500\n",
      "Epoch: 221/500- Training Loss: 1.187928318977356; Testing Loss: 1.213106632232666\n",
      "Epoch: 222/500\n",
      "Epoch: 222/500- Training Loss: 1.1891264915466309; Testing Loss: 1.2165238857269287\n",
      "Epoch: 223/500\n",
      "Epoch: 223/500- Training Loss: 1.1901079416275024; Testing Loss: 1.213087558746338\n",
      "Epoch: 224/500\n",
      "Epoch: 224/500- Training Loss: 1.185511589050293; Testing Loss: 1.2124415636062622\n",
      "Epoch: 225/500\n",
      "Epoch: 225/500- Training Loss: 1.1877830028533936; Testing Loss: 1.2103722095489502\n",
      "Epoch: 226/500\n",
      "Epoch: 226/500- Training Loss: 1.188765287399292; Testing Loss: 1.2107688188552856\n",
      "Epoch: 227/500\n",
      "Epoch: 227/500- Training Loss: 1.1854346990585327; Testing Loss: 1.2137027978897095\n",
      "Epoch: 228/500\n",
      "Epoch: 228/500- Training Loss: 1.1881844997406006; Testing Loss: 1.210923433303833\n",
      "Epoch: 229/500\n",
      "Epoch: 229/500- Training Loss: 1.1851511001586914; Testing Loss: 1.2118074893951416\n",
      "Epoch: 230/500\n",
      "Epoch: 230/500- Training Loss: 1.1858952045440674; Testing Loss: 1.2103110551834106\n",
      "Epoch: 231/500\n",
      "Epoch: 231/500- Training Loss: 1.1842875480651855; Testing Loss: 1.2137150764465332\n",
      "Epoch: 232/500\n",
      "Epoch: 232/500- Training Loss: 1.1873091459274292; Testing Loss: 1.2100803852081299\n",
      "Epoch: 233/500\n",
      "Epoch: 233/500- Training Loss: 1.183505654335022; Testing Loss: 1.2112804651260376\n",
      "Epoch: 234/500\n",
      "Epoch: 234/500- Training Loss: 1.1852620840072632; Testing Loss: 1.2103300094604492\n",
      "Epoch: 235/500\n",
      "Epoch: 235/500- Training Loss: 1.1845873594284058; Testing Loss: 1.212080717086792\n",
      "Epoch: 236/500\n",
      "Epoch: 236/500- Training Loss: 1.1853052377700806; Testing Loss: 1.2092288732528687\n",
      "Epoch: 237/500\n",
      "Epoch: 237/500- Training Loss: 1.184328556060791; Testing Loss: 1.2102543115615845\n",
      "Epoch: 238/500\n",
      "Epoch: 238/500- Training Loss: 1.186379313468933; Testing Loss: 1.2052196264266968\n",
      "Epoch: 239/500\n",
      "Epoch: 239/500- Training Loss: 1.1809808015823364; Testing Loss: 1.211877465248108\n",
      "Epoch: 240/500\n",
      "Epoch: 240/500- Training Loss: 1.1859047412872314; Testing Loss: 1.2164483070373535\n",
      "Validation accuracy: 0.6860936923644412.\n",
      "Epoch: 241/500\n",
      "Epoch: 241/500- Training Loss: 1.1875317096710205; Testing Loss: 1.2123337984085083\n",
      "Epoch: 242/500\n",
      "Epoch: 242/500- Training Loss: 1.1837341785430908; Testing Loss: 1.2106105089187622\n",
      "Epoch: 243/500\n",
      "Epoch: 243/500- Training Loss: 1.1849356889724731; Testing Loss: 1.2146326303482056\n",
      "Epoch: 244/500\n",
      "Epoch: 244/500- Training Loss: 1.1841133832931519; Testing Loss: 1.2160950899124146\n",
      "Epoch: 245/500\n",
      "Epoch: 245/500- Training Loss: 1.185502052307129; Testing Loss: 1.2085782289505005\n",
      "Epoch: 246/500\n",
      "Epoch: 246/500- Training Loss: 1.1845513582229614; Testing Loss: 1.212458610534668\n",
      "Epoch: 247/500\n",
      "Epoch: 247/500- Training Loss: 1.1872031688690186; Testing Loss: 1.2113832235336304\n",
      "Epoch: 248/500\n",
      "Epoch: 248/500- Training Loss: 1.1857988834381104; Testing Loss: 1.211293339729309\n",
      "Epoch: 249/500\n",
      "Epoch: 249/500- Training Loss: 1.1838090419769287; Testing Loss: 1.211436152458191\n",
      "Epoch: 250/500\n",
      "Epoch: 250/500- Training Loss: 1.18126380443573; Testing Loss: 1.209307074546814\n",
      "Epoch: 251/500\n",
      "Epoch: 251/500- Training Loss: 1.1813114881515503; Testing Loss: 1.2107943296432495\n",
      "Epoch: 252/500\n",
      "Epoch: 252/500- Training Loss: 1.1829957962036133; Testing Loss: 1.2094451189041138\n",
      "Epoch: 253/500\n",
      "Epoch: 253/500- Training Loss: 1.181589961051941; Testing Loss: 1.20782470703125\n",
      "Epoch: 254/500\n",
      "Epoch: 254/500- Training Loss: 1.179530382156372; Testing Loss: 1.21354079246521\n",
      "Epoch: 255/500\n",
      "Epoch: 255/500- Training Loss: 1.186486840248108; Testing Loss: 1.209688663482666\n",
      "Epoch: 256/500\n",
      "Epoch: 256/500- Training Loss: 1.1809812784194946; Testing Loss: 1.2140010595321655\n",
      "Epoch: 257/500\n",
      "Epoch: 257/500- Training Loss: 1.1840420961380005; Testing Loss: 1.2080155611038208\n",
      "Epoch: 258/500\n",
      "Epoch: 258/500- Training Loss: 1.1817411184310913; Testing Loss: 1.207916259765625\n",
      "Epoch: 259/500\n",
      "Epoch: 259/500- Training Loss: 1.182160496711731; Testing Loss: 1.2061307430267334\n",
      "Epoch: 260/500\n",
      "Epoch: 260/500- Training Loss: 1.182877540588379; Testing Loss: 1.2069846391677856\n",
      "Validation accuracy: 0.6945776466248617.\n",
      "Epoch: 261/500\n",
      "Epoch: 261/500- Training Loss: 1.180385708808899; Testing Loss: 1.2111314535140991\n",
      "Epoch: 262/500\n",
      "Epoch: 262/500- Training Loss: 1.1853402853012085; Testing Loss: 1.2074294090270996\n",
      "Epoch: 263/500\n",
      "Epoch: 263/500- Training Loss: 1.1780086755752563; Testing Loss: 1.203799843788147\n",
      "Epoch: 264/500\n",
      "Epoch: 264/500- Training Loss: 1.1767222881317139; Testing Loss: 1.2035564184188843\n",
      "Epoch: 265/500\n",
      "Epoch: 265/500- Training Loss: 1.17991042137146; Testing Loss: 1.2034077644348145\n",
      "Epoch: 266/500\n",
      "Epoch: 266/500- Training Loss: 1.1764376163482666; Testing Loss: 1.2036018371582031\n",
      "Epoch: 267/500\n",
      "Epoch: 267/500- Training Loss: 1.1774934530258179; Testing Loss: 1.2019178867340088\n",
      "Epoch: 268/500\n",
      "Epoch: 268/500- Training Loss: 1.1751030683517456; Testing Loss: 1.2021058797836304\n",
      "Epoch: 269/500\n",
      "Epoch: 269/500- Training Loss: 1.1731985807418823; Testing Loss: 1.2029403448104858\n",
      "Epoch: 270/500\n",
      "Epoch: 270/500- Training Loss: 1.1768909692764282; Testing Loss: 1.2027391195297241\n",
      "Epoch: 271/500\n",
      "Epoch: 271/500- Training Loss: 1.1764413118362427; Testing Loss: 1.2040369510650635\n",
      "Epoch: 272/500\n",
      "Epoch: 272/500- Training Loss: 1.1760172843933105; Testing Loss: 1.2042906284332275\n",
      "Epoch: 273/500\n",
      "Epoch: 273/500- Training Loss: 1.1749836206436157; Testing Loss: 1.2019646167755127\n",
      "Epoch: 274/500\n",
      "Epoch: 274/500- Training Loss: 1.175849199295044; Testing Loss: 1.2027668952941895\n",
      "Epoch: 275/500\n",
      "Epoch: 275/500- Training Loss: 1.174849033355713; Testing Loss: 1.204492449760437\n",
      "Epoch: 276/500\n",
      "Epoch: 276/500- Training Loss: 1.175527811050415; Testing Loss: 1.2007721662521362\n",
      "Epoch: 277/500\n",
      "Epoch: 277/500- Training Loss: 1.1736611127853394; Testing Loss: 1.2020361423492432\n",
      "Epoch: 278/500\n",
      "Epoch: 278/500- Training Loss: 1.1724539995193481; Testing Loss: 1.1999841928482056\n",
      "Epoch: 279/500\n",
      "Epoch: 279/500- Training Loss: 1.1714503765106201; Testing Loss: 1.2006094455718994\n",
      "Epoch: 280/500\n",
      "Epoch: 280/500- Training Loss: 1.1748889684677124; Testing Loss: 1.202478051185608\n",
      "Validation accuracy: 0.6988196237550719.\n",
      "Epoch: 281/500\n",
      "Epoch: 281/500- Training Loss: 1.1705124378204346; Testing Loss: 1.2025219202041626\n",
      "Epoch: 282/500\n",
      "Epoch: 282/500- Training Loss: 1.173191785812378; Testing Loss: 1.1991629600524902\n",
      "Epoch: 283/500\n",
      "Epoch: 283/500- Training Loss: 1.1712322235107422; Testing Loss: 1.198055624961853\n",
      "Epoch: 284/500\n",
      "Epoch: 284/500- Training Loss: 1.168906807899475; Testing Loss: 1.1999579668045044\n",
      "Epoch: 285/500\n",
      "Epoch: 285/500- Training Loss: 1.1712923049926758; Testing Loss: 1.1989535093307495\n",
      "Epoch: 286/500\n",
      "Epoch: 286/500- Training Loss: 1.1711522340774536; Testing Loss: 1.204196810722351\n",
      "Epoch: 287/500\n",
      "Epoch: 287/500- Training Loss: 1.1760563850402832; Testing Loss: 1.2043237686157227\n",
      "Epoch: 288/500\n",
      "Epoch: 288/500- Training Loss: 1.1755473613739014; Testing Loss: 1.1976948976516724\n",
      "Epoch: 289/500\n",
      "Epoch: 289/500- Training Loss: 1.168943166732788; Testing Loss: 1.2013170719146729\n",
      "Epoch: 290/500\n",
      "Epoch: 290/500- Training Loss: 1.1733893156051636; Testing Loss: 1.2001831531524658\n",
      "Epoch: 291/500\n",
      "Epoch: 291/500- Training Loss: 1.174014687538147; Testing Loss: 1.198992133140564\n",
      "Epoch: 292/500\n",
      "Epoch: 292/500- Training Loss: 1.1697918176651; Testing Loss: 1.2013579607009888\n",
      "Epoch: 293/500\n",
      "Epoch: 293/500- Training Loss: 1.1713148355484009; Testing Loss: 1.195949673652649\n",
      "Epoch: 294/500\n",
      "Epoch: 294/500- Training Loss: 1.1727014780044556; Testing Loss: 1.1969051361083984\n",
      "Epoch: 295/500\n",
      "Epoch: 295/500- Training Loss: 1.1697853803634644; Testing Loss: 1.1997146606445312\n",
      "Epoch: 296/500\n",
      "Epoch: 296/500- Training Loss: 1.1723501682281494; Testing Loss: 1.1960738897323608\n",
      "Epoch: 297/500\n",
      "Epoch: 297/500- Training Loss: 1.1700973510742188; Testing Loss: 1.200886607170105\n",
      "Epoch: 298/500\n",
      "Epoch: 298/500- Training Loss: 1.172531247138977; Testing Loss: 1.199566125869751\n",
      "Epoch: 299/500\n",
      "Epoch: 299/500- Training Loss: 1.1693414449691772; Testing Loss: 1.1952528953552246\n",
      "Epoch: 300/500\n",
      "Epoch: 300/500- Training Loss: 1.1689372062683105; Testing Loss: 1.1951698064804077\n",
      "Validation accuracy: 0.7080413131685724.\n",
      "Epoch: 301/500\n",
      "Epoch: 301/500- Training Loss: 1.1657122373580933; Testing Loss: 1.1956110000610352\n",
      "Epoch: 302/500\n",
      "Epoch: 302/500- Training Loss: 1.1663988828659058; Testing Loss: 1.1974431276321411\n",
      "Epoch: 303/500\n",
      "Epoch: 303/500- Training Loss: 1.1686956882476807; Testing Loss: 1.195520281791687\n",
      "Epoch: 304/500\n",
      "Epoch: 304/500- Training Loss: 1.1666537523269653; Testing Loss: 1.1939902305603027\n",
      "Epoch: 305/500\n",
      "Epoch: 305/500- Training Loss: 1.1658943891525269; Testing Loss: 1.197641372680664\n",
      "Epoch: 306/500\n",
      "Epoch: 306/500- Training Loss: 1.1677680015563965; Testing Loss: 1.1957001686096191\n",
      "Epoch: 307/500\n",
      "Epoch: 307/500- Training Loss: 1.1664888858795166; Testing Loss: 1.1970515251159668\n",
      "Epoch: 308/500\n",
      "Epoch: 308/500- Training Loss: 1.163953423500061; Testing Loss: 1.1993757486343384\n",
      "Epoch: 309/500\n",
      "Epoch: 309/500- Training Loss: 1.1674518585205078; Testing Loss: 1.198123574256897\n",
      "Epoch: 310/500\n",
      "Epoch: 310/500- Training Loss: 1.167356252670288; Testing Loss: 1.1993651390075684\n",
      "Epoch: 311/500\n",
      "Epoch: 311/500- Training Loss: 1.168646216392517; Testing Loss: 1.1941425800323486\n",
      "Epoch: 312/500\n",
      "Epoch: 312/500- Training Loss: 1.1652510166168213; Testing Loss: 1.1967395544052124\n",
      "Epoch: 313/500\n",
      "Epoch: 313/500- Training Loss: 1.1695064306259155; Testing Loss: 1.205496072769165\n",
      "Epoch: 314/500\n",
      "Epoch: 314/500- Training Loss: 1.1736831665039062; Testing Loss: 1.2017914056777954\n",
      "Epoch: 315/500\n",
      "Epoch: 315/500- Training Loss: 1.166856288909912; Testing Loss: 1.2017219066619873\n",
      "Epoch: 316/500\n",
      "Epoch: 316/500- Training Loss: 1.1661453247070312; Testing Loss: 1.2015072107315063\n",
      "Epoch: 317/500\n",
      "Epoch: 317/500- Training Loss: 1.1666781902313232; Testing Loss: 1.1936304569244385\n",
      "Epoch: 318/500\n",
      "Epoch: 318/500- Training Loss: 1.1656646728515625; Testing Loss: 1.195321798324585\n",
      "Epoch: 319/500\n",
      "Epoch: 319/500- Training Loss: 1.1671956777572632; Testing Loss: 1.1931105852127075\n",
      "Epoch: 320/500\n",
      "Epoch: 320/500- Training Loss: 1.1641730070114136; Testing Loss: 1.2001744508743286\n",
      "Validation accuracy: 0.7034304684618222.\n",
      "Epoch: 321/500\n",
      "Epoch: 321/500- Training Loss: 1.1700167655944824; Testing Loss: 1.203395128250122\n",
      "Epoch: 322/500\n",
      "Epoch: 322/500- Training Loss: 1.1719917058944702; Testing Loss: 1.2009540796279907\n",
      "Epoch: 323/500\n",
      "Epoch: 323/500- Training Loss: 1.1675872802734375; Testing Loss: 1.2045509815216064\n",
      "Epoch: 324/500\n",
      "Epoch: 324/500- Training Loss: 1.1710976362228394; Testing Loss: 1.2035671472549438\n",
      "Epoch: 325/500\n",
      "Epoch: 325/500- Training Loss: 1.1703342199325562; Testing Loss: 1.1934770345687866\n",
      "Epoch: 326/500\n",
      "Epoch: 326/500- Training Loss: 1.166080117225647; Testing Loss: 1.199783444404602\n",
      "Epoch: 327/500\n",
      "Epoch: 327/500- Training Loss: 1.1696832180023193; Testing Loss: 1.1932272911071777\n",
      "Epoch: 328/500\n",
      "Epoch: 328/500- Training Loss: 1.1652933359146118; Testing Loss: 1.1979268789291382\n",
      "Epoch: 329/500\n",
      "Epoch: 329/500- Training Loss: 1.1678391695022583; Testing Loss: 1.1983654499053955\n",
      "Epoch: 330/500\n",
      "Epoch: 330/500- Training Loss: 1.1673704385757446; Testing Loss: 1.1978299617767334\n",
      "Epoch: 331/500\n",
      "Epoch: 331/500- Training Loss: 1.1637526750564575; Testing Loss: 1.199156403541565\n",
      "Epoch: 332/500\n",
      "Epoch: 332/500- Training Loss: 1.1671475172042847; Testing Loss: 1.1935887336730957\n",
      "Epoch: 333/500\n",
      "Epoch: 333/500- Training Loss: 1.1642779111862183; Testing Loss: 1.195457100868225\n",
      "Epoch: 334/500\n",
      "Epoch: 334/500- Training Loss: 1.1711690425872803; Testing Loss: 1.1921281814575195\n",
      "Epoch: 335/500\n",
      "Epoch: 335/500- Training Loss: 1.1666775941848755; Testing Loss: 1.1962826251983643\n",
      "Epoch: 336/500\n",
      "Epoch: 336/500- Training Loss: 1.1707876920700073; Testing Loss: 1.1947706937789917\n",
      "Epoch: 337/500\n",
      "Epoch: 337/500- Training Loss: 1.1658332347869873; Testing Loss: 1.195812702178955\n",
      "Epoch: 338/500\n",
      "Epoch: 338/500- Training Loss: 1.1695220470428467; Testing Loss: 1.1979578733444214\n",
      "Epoch: 339/500\n",
      "Epoch: 339/500- Training Loss: 1.168121337890625; Testing Loss: 1.1934038400650024\n",
      "Epoch: 340/500\n",
      "Epoch: 340/500- Training Loss: 1.1627894639968872; Testing Loss: 1.1932510137557983\n",
      "Validation accuracy: 0.7095167834747326.\n",
      "Epoch: 341/500\n",
      "Epoch: 341/500- Training Loss: 1.1664578914642334; Testing Loss: 1.1895016431808472\n",
      "Epoch: 342/500\n",
      "Epoch: 342/500- Training Loss: 1.1626157760620117; Testing Loss: 1.191436767578125\n",
      "Epoch: 343/500\n",
      "Epoch: 343/500- Training Loss: 1.1628623008728027; Testing Loss: 1.190633773803711\n",
      "Epoch: 344/500\n",
      "Epoch: 344/500- Training Loss: 1.1598727703094482; Testing Loss: 1.1946295499801636\n",
      "Epoch: 345/500\n",
      "Epoch: 345/500- Training Loss: 1.1620752811431885; Testing Loss: 1.1946395635604858\n",
      "Epoch: 346/500\n",
      "Epoch: 346/500- Training Loss: 1.1620545387268066; Testing Loss: 1.1920139789581299\n",
      "Epoch: 347/500\n",
      "Epoch: 347/500- Training Loss: 1.1607611179351807; Testing Loss: 1.1926814317703247\n",
      "Epoch: 348/500\n",
      "Epoch: 348/500- Training Loss: 1.1594390869140625; Testing Loss: 1.1916632652282715\n",
      "Epoch: 349/500\n",
      "Epoch: 349/500- Training Loss: 1.159793496131897; Testing Loss: 1.1924632787704468\n",
      "Epoch: 350/500\n",
      "Epoch: 350/500- Training Loss: 1.1609957218170166; Testing Loss: 1.1914926767349243\n",
      "Epoch: 351/500\n",
      "Epoch: 351/500- Training Loss: 1.1606563329696655; Testing Loss: 1.1897306442260742\n",
      "Epoch: 352/500\n",
      "Epoch: 352/500- Training Loss: 1.1586710214614868; Testing Loss: 1.1898871660232544\n",
      "Epoch: 353/500\n",
      "Epoch: 353/500- Training Loss: 1.161486029624939; Testing Loss: 1.1902079582214355\n",
      "Epoch: 354/500\n",
      "Epoch: 354/500- Training Loss: 1.1604034900665283; Testing Loss: 1.1916145086288452\n",
      "Epoch: 355/500\n",
      "Epoch: 355/500- Training Loss: 1.160571575164795; Testing Loss: 1.1888110637664795\n",
      "Epoch: 356/500\n",
      "Epoch: 356/500- Training Loss: 1.161219596862793; Testing Loss: 1.1887437105178833\n",
      "Epoch: 357/500\n",
      "Epoch: 357/500- Training Loss: 1.1624841690063477; Testing Loss: 1.1869114637374878\n",
      "Epoch: 358/500\n",
      "Epoch: 358/500- Training Loss: 1.157956838607788; Testing Loss: 1.1865359544754028\n",
      "Epoch: 359/500\n",
      "Epoch: 359/500- Training Loss: 1.1603692770004272; Testing Loss: 1.1899218559265137\n",
      "Epoch: 360/500\n",
      "Epoch: 360/500- Training Loss: 1.1620361804962158; Testing Loss: 1.188236117362976\n",
      "Validation accuracy: 0.7146809295462929.\n",
      "Epoch: 361/500\n",
      "Epoch: 361/500- Training Loss: 1.1587461233139038; Testing Loss: 1.1931042671203613\n",
      "Epoch: 362/500\n",
      "Epoch: 362/500- Training Loss: 1.1647446155548096; Testing Loss: 1.1971055269241333\n",
      "Epoch: 363/500\n",
      "Epoch: 363/500- Training Loss: 1.1695396900177002; Testing Loss: 1.1921137571334839\n",
      "Epoch: 364/500\n",
      "Epoch: 364/500- Training Loss: 1.1621712446212769; Testing Loss: 1.194398283958435\n",
      "Epoch: 365/500\n",
      "Epoch: 365/500- Training Loss: 1.1640379428863525; Testing Loss: 1.1905697584152222\n",
      "Epoch: 366/500\n",
      "Epoch: 366/500- Training Loss: 1.1582705974578857; Testing Loss: 1.1935172080993652\n",
      "Epoch: 367/500\n",
      "Epoch: 367/500- Training Loss: 1.1630780696868896; Testing Loss: 1.1879609823226929\n",
      "Epoch: 368/500\n",
      "Epoch: 368/500- Training Loss: 1.1603554487228394; Testing Loss: 1.186783790588379\n",
      "Epoch: 369/500\n",
      "Epoch: 369/500- Training Loss: 1.1590297222137451; Testing Loss: 1.1883944272994995\n",
      "Epoch: 370/500\n",
      "Epoch: 370/500- Training Loss: 1.1599597930908203; Testing Loss: 1.1856306791305542\n",
      "Epoch: 371/500\n",
      "Epoch: 371/500- Training Loss: 1.1552306413650513; Testing Loss: 1.194629430770874\n",
      "Epoch: 372/500\n",
      "Epoch: 372/500- Training Loss: 1.1624964475631714; Testing Loss: 1.1909611225128174\n",
      "Epoch: 373/500\n",
      "Epoch: 373/500- Training Loss: 1.1613783836364746; Testing Loss: 1.1902698278427124\n",
      "Epoch: 374/500\n",
      "Epoch: 374/500- Training Loss: 1.1579415798187256; Testing Loss: 1.1920429468154907\n",
      "Epoch: 375/500\n",
      "Epoch: 375/500- Training Loss: 1.1645824909210205; Testing Loss: 1.185625433921814\n",
      "Epoch: 376/500\n",
      "Epoch: 376/500- Training Loss: 1.1589322090148926; Testing Loss: 1.1911134719848633\n",
      "Epoch: 377/500\n",
      "Epoch: 377/500- Training Loss: 1.1643693447113037; Testing Loss: 1.1852269172668457\n",
      "Epoch: 378/500\n",
      "Epoch: 378/500- Training Loss: 1.1574000120162964; Testing Loss: 1.1888623237609863\n",
      "Epoch: 379/500\n",
      "Epoch: 379/500- Training Loss: 1.1659858226776123; Testing Loss: 1.1920017004013062\n",
      "Epoch: 380/500\n",
      "Epoch: 380/500- Training Loss: 1.161676049232483; Testing Loss: 1.1927931308746338\n",
      "Validation accuracy: 0.7109922537808927.\n",
      "Epoch: 381/500\n",
      "Epoch: 381/500- Training Loss: 1.1622462272644043; Testing Loss: 1.1913092136383057\n",
      "Epoch: 382/500\n",
      "Epoch: 382/500- Training Loss: 1.1651266813278198; Testing Loss: 1.187054991722107\n",
      "Epoch: 383/500\n",
      "Epoch: 383/500- Training Loss: 1.1585774421691895; Testing Loss: 1.1915793418884277\n",
      "Epoch: 384/500\n",
      "Epoch: 384/500- Training Loss: 1.1621571779251099; Testing Loss: 1.1867040395736694\n",
      "Epoch: 385/500\n",
      "Epoch: 385/500- Training Loss: 1.1601861715316772; Testing Loss: 1.1876791715621948\n",
      "Epoch: 386/500\n",
      "Epoch: 386/500- Training Loss: 1.1613893508911133; Testing Loss: 1.1877535581588745\n",
      "Epoch: 387/500\n",
      "Epoch: 387/500- Training Loss: 1.1574277877807617; Testing Loss: 1.1928013563156128\n",
      "Epoch: 388/500\n",
      "Epoch: 388/500- Training Loss: 1.1590136289596558; Testing Loss: 1.192179560661316\n",
      "Epoch: 389/500\n",
      "Epoch: 389/500- Training Loss: 1.1567933559417725; Testing Loss: 1.194338321685791\n",
      "Epoch: 390/500\n",
      "Epoch: 390/500- Training Loss: 1.1617227792739868; Testing Loss: 1.1910343170166016\n",
      "Epoch: 391/500\n",
      "Epoch: 391/500- Training Loss: 1.158308744430542; Testing Loss: 1.191139817237854\n",
      "Epoch: 392/500\n",
      "Epoch: 392/500- Training Loss: 1.1589514017105103; Testing Loss: 1.1868542432785034\n",
      "Epoch: 393/500\n",
      "Epoch: 393/500- Training Loss: 1.159070372581482; Testing Loss: 1.185416340827942\n",
      "Epoch: 394/500\n",
      "Epoch: 394/500- Training Loss: 1.1570771932601929; Testing Loss: 1.1852682828903198\n",
      "Epoch: 395/500\n",
      "Epoch: 395/500- Training Loss: 1.1573255062103271; Testing Loss: 1.1830673217773438\n",
      "Epoch: 396/500\n",
      "Epoch: 396/500- Training Loss: 1.15739107131958; Testing Loss: 1.1857399940490723\n",
      "Epoch: 397/500\n",
      "Epoch: 397/500- Training Loss: 1.154675006866455; Testing Loss: 1.1842081546783447\n",
      "Epoch: 398/500\n",
      "Epoch: 398/500- Training Loss: 1.1539582014083862; Testing Loss: 1.1840505599975586\n",
      "Epoch: 399/500\n",
      "Epoch: 399/500- Training Loss: 1.1552432775497437; Testing Loss: 1.182858943939209\n",
      "Epoch: 400/500\n",
      "Epoch: 400/500- Training Loss: 1.1543232202529907; Testing Loss: 1.1822303533554077\n",
      "Validation accuracy: 0.7216894135005533.\n",
      "Epoch: 401/500\n",
      "Epoch: 401/500- Training Loss: 1.1529860496520996; Testing Loss: 1.187198281288147\n",
      "Epoch: 402/500\n",
      "Epoch: 402/500- Training Loss: 1.1561028957366943; Testing Loss: 1.1873021125793457\n",
      "Epoch: 403/500\n",
      "Epoch: 403/500- Training Loss: 1.1564271450042725; Testing Loss: 1.1863198280334473\n",
      "Epoch: 404/500\n",
      "Epoch: 404/500- Training Loss: 1.1528232097625732; Testing Loss: 1.1859636306762695\n",
      "Epoch: 405/500\n",
      "Epoch: 405/500- Training Loss: 1.1555354595184326; Testing Loss: 1.1837856769561768\n",
      "Epoch: 406/500\n",
      "Epoch: 406/500- Training Loss: 1.1546931266784668; Testing Loss: 1.185904622077942\n",
      "Epoch: 407/500\n",
      "Epoch: 407/500- Training Loss: 1.15492844581604; Testing Loss: 1.1851378679275513\n",
      "Epoch: 408/500\n",
      "Epoch: 408/500- Training Loss: 1.1522109508514404; Testing Loss: 1.1848468780517578\n",
      "Epoch: 409/500\n",
      "Epoch: 409/500- Training Loss: 1.1541359424591064; Testing Loss: 1.186222791671753\n",
      "Epoch: 410/500\n",
      "Epoch: 410/500- Training Loss: 1.152086853981018; Testing Loss: 1.1838223934173584\n",
      "Epoch: 411/500\n",
      "Epoch: 411/500- Training Loss: 1.1562951803207397; Testing Loss: 1.1837915182113647\n",
      "Epoch: 412/500\n",
      "Epoch: 412/500- Training Loss: 1.1533631086349487; Testing Loss: 1.1836761236190796\n",
      "Epoch: 413/500\n",
      "Epoch: 413/500- Training Loss: 1.1568231582641602; Testing Loss: 1.1811027526855469\n",
      "Epoch: 414/500\n",
      "Epoch: 414/500- Training Loss: 1.1537158489227295; Testing Loss: 1.1824978590011597\n",
      "Epoch: 415/500\n",
      "Epoch: 415/500- Training Loss: 1.1534110307693481; Testing Loss: 1.183768630027771\n",
      "Epoch: 416/500\n",
      "Epoch: 416/500- Training Loss: 1.1505635976791382; Testing Loss: 1.1868714094161987\n",
      "Epoch: 417/500\n",
      "Epoch: 417/500- Training Loss: 1.1523722410202026; Testing Loss: 1.1832857131958008\n",
      "Epoch: 418/500\n",
      "Epoch: 418/500- Training Loss: 1.1538065671920776; Testing Loss: 1.1812382936477661\n",
      "Epoch: 419/500\n",
      "Epoch: 419/500- Training Loss: 1.1533886194229126; Testing Loss: 1.1829700469970703\n",
      "Epoch: 420/500\n",
      "Epoch: 420/500- Training Loss: 1.1521376371383667; Testing Loss: 1.1819320917129517\n",
      "Validation accuracy: 0.7202139431943932.\n",
      "Epoch: 421/500\n",
      "Epoch: 421/500- Training Loss: 1.151828408241272; Testing Loss: 1.1807663440704346\n",
      "Epoch: 422/500\n",
      "Epoch: 422/500- Training Loss: 1.1523507833480835; Testing Loss: 1.1827870607376099\n",
      "Epoch: 423/500\n",
      "Epoch: 423/500- Training Loss: 1.153529167175293; Testing Loss: 1.1784319877624512\n",
      "Epoch: 424/500\n",
      "Epoch: 424/500- Training Loss: 1.1524930000305176; Testing Loss: 1.177815318107605\n",
      "Epoch: 425/500\n",
      "Epoch: 425/500- Training Loss: 1.1506837606430054; Testing Loss: 1.180996060371399\n",
      "Epoch: 426/500\n",
      "Epoch: 426/500- Training Loss: 1.1508904695510864; Testing Loss: 1.178011417388916\n",
      "Epoch: 427/500\n",
      "Epoch: 427/500- Training Loss: 1.1517159938812256; Testing Loss: 1.1783932447433472\n",
      "Epoch: 428/500\n",
      "Epoch: 428/500- Training Loss: 1.1527645587921143; Testing Loss: 1.1763395071029663\n",
      "Epoch: 429/500\n",
      "Epoch: 429/500- Training Loss: 1.1508007049560547; Testing Loss: 1.1812787055969238\n",
      "Epoch: 430/500\n",
      "Epoch: 430/500- Training Loss: 1.1512932777404785; Testing Loss: 1.1830135583877563\n",
      "Epoch: 431/500\n",
      "Epoch: 431/500- Training Loss: 1.1534632444381714; Testing Loss: 1.1814942359924316\n",
      "Epoch: 432/500\n",
      "Epoch: 432/500- Training Loss: 1.1521382331848145; Testing Loss: 1.1788243055343628\n",
      "Epoch: 433/500\n",
      "Epoch: 433/500- Training Loss: 1.1489427089691162; Testing Loss: 1.1817346811294556\n",
      "Epoch: 434/500\n",
      "Epoch: 434/500- Training Loss: 1.1547496318817139; Testing Loss: 1.181618094444275\n",
      "Epoch: 435/500\n",
      "Epoch: 435/500- Training Loss: 1.1501492261886597; Testing Loss: 1.1861549615859985\n",
      "Epoch: 436/500\n",
      "Epoch: 436/500- Training Loss: 1.1525216102600098; Testing Loss: 1.1826725006103516\n",
      "Epoch: 437/500\n",
      "Epoch: 437/500- Training Loss: 1.1496036052703857; Testing Loss: 1.1791545152664185\n",
      "Epoch: 438/500\n",
      "Epoch: 438/500- Training Loss: 1.1485873460769653; Testing Loss: 1.1796413660049438\n",
      "Epoch: 439/500\n",
      "Epoch: 439/500- Training Loss: 1.150425910949707; Testing Loss: 1.1805486679077148\n",
      "Epoch: 440/500\n",
      "Epoch: 440/500- Training Loss: 1.1507309675216675; Testing Loss: 1.177543044090271\n",
      "Validation accuracy: 0.7259313906307635.\n",
      "Epoch: 441/500\n",
      "Epoch: 441/500- Training Loss: 1.151149034500122; Testing Loss: 1.1754246950149536\n",
      "Epoch: 442/500\n",
      "Epoch: 442/500- Training Loss: 1.1506619453430176; Testing Loss: 1.1796962022781372\n",
      "Epoch: 443/500\n",
      "Epoch: 443/500- Training Loss: 1.150931715965271; Testing Loss: 1.1733986139297485\n",
      "Epoch: 444/500\n",
      "Epoch: 444/500- Training Loss: 1.1502376794815063; Testing Loss: 1.173749566078186\n",
      "Epoch: 445/500\n",
      "Epoch: 445/500- Training Loss: 1.1519361734390259; Testing Loss: 1.1847037076950073\n",
      "Epoch: 446/500\n",
      "Epoch: 446/500- Training Loss: 1.1549179553985596; Testing Loss: 1.1757469177246094\n",
      "Epoch: 447/500\n",
      "Epoch: 447/500- Training Loss: 1.1489850282669067; Testing Loss: 1.179291844367981\n",
      "Epoch: 448/500\n",
      "Epoch: 448/500- Training Loss: 1.1554087400436401; Testing Loss: 1.1816703081130981\n",
      "Epoch: 449/500\n",
      "Epoch: 449/500- Training Loss: 1.1545531749725342; Testing Loss: 1.1766529083251953\n",
      "Epoch: 450/500\n",
      "Epoch: 450/500- Training Loss: 1.1480199098587036; Testing Loss: 1.1815078258514404\n",
      "Epoch: 451/500\n",
      "Epoch: 451/500- Training Loss: 1.1540943384170532; Testing Loss: 1.1765142679214478\n",
      "Epoch: 452/500\n",
      "Epoch: 452/500- Training Loss: 1.1494009494781494; Testing Loss: 1.1812437772750854\n",
      "Epoch: 453/500\n",
      "Epoch: 453/500- Training Loss: 1.1559242010116577; Testing Loss: 1.177359938621521\n",
      "Epoch: 454/500\n",
      "Epoch: 454/500- Training Loss: 1.1496189832687378; Testing Loss: 1.1787166595458984\n",
      "Epoch: 455/500\n",
      "Epoch: 455/500- Training Loss: 1.1475694179534912; Testing Loss: 1.1806285381317139\n",
      "Epoch: 456/500\n",
      "Epoch: 456/500- Training Loss: 1.1501524448394775; Testing Loss: 1.1791239976882935\n",
      "Epoch: 457/500\n",
      "Epoch: 457/500- Training Loss: 1.1502165794372559; Testing Loss: 1.1776067018508911\n",
      "Epoch: 458/500\n",
      "Epoch: 458/500- Training Loss: 1.1508210897445679; Testing Loss: 1.1804677248001099\n",
      "Epoch: 459/500\n",
      "Epoch: 459/500- Training Loss: 1.1498398780822754; Testing Loss: 1.17885160446167\n",
      "Epoch: 460/500\n",
      "Epoch: 460/500- Training Loss: 1.148011326789856; Testing Loss: 1.1805789470672607\n",
      "Validation accuracy: 0.7229804500184434.\n",
      "Epoch: 461/500\n",
      "Epoch: 461/500- Training Loss: 1.1524245738983154; Testing Loss: 1.1804242134094238\n",
      "Epoch: 462/500\n",
      "Epoch: 462/500- Training Loss: 1.1508963108062744; Testing Loss: 1.1801291704177856\n",
      "Epoch: 463/500\n",
      "Epoch: 463/500- Training Loss: 1.149550437927246; Testing Loss: 1.1779658794403076\n",
      "Epoch: 464/500\n",
      "Epoch: 464/500- Training Loss: 1.1479759216308594; Testing Loss: 1.176024079322815\n",
      "Epoch: 465/500\n",
      "Epoch: 465/500- Training Loss: 1.1470084190368652; Testing Loss: 1.175268530845642\n",
      "Epoch: 466/500\n",
      "Epoch: 466/500- Training Loss: 1.147648811340332; Testing Loss: 1.1748871803283691\n",
      "Epoch: 467/500\n",
      "Epoch: 467/500- Training Loss: 1.1451022624969482; Testing Loss: 1.1771939992904663\n",
      "Epoch: 468/500\n",
      "Epoch: 468/500- Training Loss: 1.1478697061538696; Testing Loss: 1.175523281097412\n",
      "Epoch: 469/500\n",
      "Epoch: 469/500- Training Loss: 1.1462961435317993; Testing Loss: 1.1764392852783203\n",
      "Epoch: 470/500\n",
      "Epoch: 470/500- Training Loss: 1.1466498374938965; Testing Loss: 1.1765990257263184\n",
      "Epoch: 471/500\n",
      "Epoch: 471/500- Training Loss: 1.1466689109802246; Testing Loss: 1.1752877235412598\n",
      "Epoch: 472/500\n",
      "Epoch: 472/500- Training Loss: 1.144697666168213; Testing Loss: 1.1753255128860474\n",
      "Epoch: 473/500\n",
      "Epoch: 473/500- Training Loss: 1.1452622413635254; Testing Loss: 1.1740665435791016\n",
      "Epoch: 474/500\n",
      "Epoch: 474/500- Training Loss: 1.1448744535446167; Testing Loss: 1.175425410270691\n",
      "Epoch: 475/500\n",
      "Epoch: 475/500- Training Loss: 1.146690845489502; Testing Loss: 1.1716550588607788\n",
      "Epoch: 476/500\n",
      "Epoch: 476/500- Training Loss: 1.1475476026535034; Testing Loss: 1.1738423109054565\n",
      "Epoch: 477/500\n",
      "Epoch: 477/500- Training Loss: 1.149463176727295; Testing Loss: 1.1758368015289307\n",
      "Epoch: 478/500\n",
      "Epoch: 478/500- Training Loss: 1.1488243341445923; Testing Loss: 1.1750454902648926\n",
      "Epoch: 479/500\n",
      "Epoch: 479/500- Training Loss: 1.150102972984314; Testing Loss: 1.1721752882003784\n",
      "Epoch: 480/500\n",
      "Epoch: 480/500- Training Loss: 1.1490726470947266; Testing Loss: 1.1778122186660767\n",
      "Validation accuracy: 0.7257469568424936.\n",
      "Epoch: 481/500\n",
      "Epoch: 481/500- Training Loss: 1.1491098403930664; Testing Loss: 1.1748483180999756\n",
      "Epoch: 482/500\n",
      "Epoch: 482/500- Training Loss: 1.1464024782180786; Testing Loss: 1.173958659172058\n",
      "Epoch: 483/500\n",
      "Epoch: 483/500- Training Loss: 1.1465787887573242; Testing Loss: 1.174692153930664\n",
      "Epoch: 484/500\n",
      "Epoch: 484/500- Training Loss: 1.1462935209274292; Testing Loss: 1.1752371788024902\n",
      "Epoch: 485/500\n",
      "Epoch: 485/500- Training Loss: 1.1461135149002075; Testing Loss: 1.1735137701034546\n",
      "Epoch: 486/500\n",
      "Epoch: 486/500- Training Loss: 1.151025414466858; Testing Loss: 1.1745086908340454\n",
      "Epoch: 487/500\n",
      "Epoch: 487/500- Training Loss: 1.1489604711532593; Testing Loss: 1.173401117324829\n",
      "Epoch: 488/500\n",
      "Epoch: 488/500- Training Loss: 1.1503676176071167; Testing Loss: 1.1724810600280762\n",
      "Epoch: 489/500\n",
      "Epoch: 489/500- Training Loss: 1.1470472812652588; Testing Loss: 1.1734257936477661\n",
      "Epoch: 490/500\n",
      "Epoch: 490/500- Training Loss: 1.1474287509918213; Testing Loss: 1.1733604669570923\n",
      "Epoch: 491/500\n",
      "Epoch: 491/500- Training Loss: 1.1462950706481934; Testing Loss: 1.1754149198532104\n",
      "Epoch: 492/500\n",
      "Epoch: 492/500- Training Loss: 1.1477742195129395; Testing Loss: 1.1726362705230713\n",
      "Epoch: 493/500\n",
      "Epoch: 493/500- Training Loss: 1.145439624786377; Testing Loss: 1.17293381690979\n",
      "Epoch: 494/500\n",
      "Epoch: 494/500- Training Loss: 1.1457582712173462; Testing Loss: 1.1728001832962036\n",
      "Epoch: 495/500\n",
      "Epoch: 495/500- Training Loss: 1.1459765434265137; Testing Loss: 1.1755579710006714\n",
      "Epoch: 496/500\n",
      "Epoch: 496/500- Training Loss: 1.1458079814910889; Testing Loss: 1.1724013090133667\n",
      "Epoch: 497/500\n",
      "Epoch: 497/500- Training Loss: 1.1464134454727173; Testing Loss: 1.1747984886169434\n",
      "Epoch: 498/500\n",
      "Epoch: 498/500- Training Loss: 1.1482722759246826; Testing Loss: 1.1773282289505005\n",
      "Epoch: 499/500\n",
      "Epoch: 499/500- Training Loss: 1.1492141485214233; Testing Loss: 1.176653265953064\n",
      "Epoch: 500/500\n",
      "Epoch: 500/500- Training Loss: 1.15317702293396; Testing Loss: 1.1773301362991333\n",
      "Validation accuracy: 0.7257469568424936.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:53:40.127692Z",
     "start_time": "2025-03-26T13:53:39.429617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = pd.DataFrame(epoch_train_val).transpose().reset_index()\n",
    "lines = train_loss.plot.line(x='index', y=['training_loss', 'test_loss'])\n",
    "plt.title('GAT - Training and Validation Progress')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['training', 'validation'], loc='lower right')\n",
    "plt.show()"
   ],
   "id": "a722780f8e38af26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
